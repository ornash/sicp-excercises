#+TITLE: Notes on Structure and Interpretation of Computer Programs
#+STARTUP: indent
[[./sicp.html][HTML Export]]
[[https://github.com/ornash/book-notes/blob/master/sicp.org][Github Page]]


* Chapter 1 : Building Abstractions with Procedures
** Computational Process
Computational processes are abstract beings that inhabit computers. As they evolve, processes manipulate other abstract
things called data. The evolution of a process is directed by a pattern of rules called a program. People create
programs to direct processes.

** Elements of Programming
Every powerful language has three mechanisms for expressing processes:
1. *primitive expressions*, which represent the simplest entities the language is concerned with,
2. *means of combination*, by which compound elements are built from simple ones, and
3. *means of abstraction*, by which compound elements can be named and manipulated as units.

** Procedure/Program Application/Evaluation
A substition model is used for procedure application. There are two evalution methods for procedure application:
1. *Applicative-order evaluation*: evaluate the arguments and then apply. An argument is evaluated only once but that
   evaluation is going to be wasteful if the argument isn't used. Scheme uses applicative-order evaluation.
2. *Normal-order evaluation*: fully expand and then reduce/evaluate i.e. obtain expression involving only primitives and
   then evaluate. An argument isn't evaluated until it is used but it might be evaluated more than once after full expansion.

Check out exercise [[https://github.com/ornash/sicp-excercises/tree/master/chapter-1/20-applicative-vs-normal][1.20]] to understand how this choice affects evaluation.

** Procedures and the Processes They Generate
We now know the elements of programming, but that is not enough to say that we know how to program. In order to become
expert programmer, we must learn to visualize the processes generated by various types of procedures. We must also learn
to calculate the rates at which the procedure consumes the computational resources of time and space.

*** Recursive Procedures
Evaluate and visualize the processes generated by following *recursive procedures* in terms of time and space.
#+BEGIN_SRC scheme
;;Recursive procedure that generates *recursive* process.
(define (factorial n)
  (if (= n 1)
    1
    (* n (factorial (- n 1)))))

;;Recursive procedure that generates *iterative* process.
(define (factorial-iter n)
  (define (iter product counter)
    (if (> counter n)
      product
      (iter (* counter product)
            (+ counter 1))))
  (iter 1 1))  
#+END_SRC

When we consider the "shapes" of the two processes, we see that they evolve quite differently. One grows and shrinks
while other is lean. Carrying out the *recursive* process requires that the interpreter keep track of some *hidden*
information and operations to be performed later on. On the other hand, carrying out the *iterative* process does not
require processing any hidden information, its state can be summarized by fixed number of state variables in the
procedure.

The implementation that generates iterative process is called as *tail recursive* implementation. In tail recursive
implementation the last statement is a call to recursive procedure itself with no work left to be performed in current
procedure call; therefore, compiler can optimize such call by replacing current stack frame instead of creating a new
one.

*** Tree Recursion
Evaluate and visualize the processes generated by following *recursive procedures* in terms of time and space.
#+BEGIN_SRC scheme
;;Recursive procedure that generates *tree recursive* process.
(define (fib n)
  (cond ((= n 0) 0)
        ((= n 1) 1)
        (else (+ (fib (- n 1)) (fib (- n 2))))
  ))

;;Recursive procedure that generates *iterative* process.
(define (fib-iter n)
  (define (iter a b count)
    (cond ((= n 0) 0)
          ((= n 1) 1)
          ((= count n) a)
          (else (iter (+ a b) a (+ count 1)))))
   (iter 1 0 1))
#+END_SRC

Tree recursion is a powerful tool to operate on hierarchically structured data.

*** Decomposition
Functional procedures encourage reuse because every computation unit is defined as a function which can be used somewhere
else. Decomposition into units also helps transform one implementation into other quickly.

*** Notes
- Functional programming makes you think in terms of functions/tasks and not in terms of data/elements e.g. Compare the
  difference in implementation of the following in imperative programming. Use wishful thinking and abstraction to
  program better and faster.
#+BEGIN_SRC scheme
(define (smallest-divisor n)
  (find-divisor n 2))

(define (find-divisor n test-divisor)
  (cond ((> (square test-divisor) n) n)
        ((divides? test-divisor n) test-divisor)
        (else (find-divisor n (+ test-divisor 1)))))

(define (divides? denom numer)
  (= (remainder numer denom) 0))
#+END_SRC
- Be careful of recursive procedures, subtle changes in code can result in huge difference in time
  complexity. e.g. Check exercise [[./chapter-1/22-search-primes/22-search-primes.scm][1.25]] and [[./chapter-1/22-search-primes/22-search-primes.scm][1.26]].
** Formulating Abstractions with Higher Order Procedures
Procedures that manipulate procedures are called higher-order procedures i.e. procedures that can accept procedures as
arguments or return procedures as values. As shown below higher-order procedures permit us to manipulate general methods
to create further abstractions.

*** Procedure as Argument
#+BEGIN_SRC scheme
(define (sum operation a next b)
  (if (> a b)
      0
      (+ (operation a)
         (sum operation (next a) next b))))

(define (inc n) (+ n 1))

(define (cube x) (* x x x))

(define (sum-cubes a b)
  (sum cube a inc b))

(define (identity x) x)

(define (sum-integers a b)
  (sum identity a inc b))
#+END_SRC

Note: This simple feature of allowing procedures as arguments is extremely powerful and helps us to identify and share
common underlying patterns in various programs thereby reducing overall code and effort. It allows to create higher
levels of abstraction helping us think better and faster. This is either not possible or very cumbersome in other
languages. 

*** Constructing Procedures Using Lambda
(lambda (<formal-parameters>) <body>)

#+BEGIN_SRC scheme
(lambda (x) (+ x 1))

(define (sum-cubes a b)
  (sum cube a (lambda (x) (+ x 1)) b))
#+END_SRC

- Lambda function's body is usually defined in terms existing functions.
- Lambda function can itself define and use other lambda functions in its body.
- Lambda function can itself expect other functions as parameters and use them in its body.
- Functions can define lambdas on the fly and apply them.
- Functions can define lambdas on the fly and pass them as arguments.
- When a function receives a lambda function as parameter it gets a name and that named function can itself be applied
  and evaluated.
*** Using let To Create Local Variable
Let allows one to bind variables as locally as possible to where they are to be used. The general form of a let
expression is

(let ((<var1> <exp1>)
      (<var2> <exp2>)
      ...
      (<varN> <expN>))
  <body>)
  
which can be thought of as saying

let <var1> have the value <exp1> and
    <var2> have the value <exp2> and
    ...
    <varN> have the value <expN>
  in <body>
#+BEGIN_SRC scheme
(define (f x y)
  (let ((a (+ 1 (* x y)))
        (b (- 1 y)))
    (+ (* x (square a))
       (* y b)
       (* a b))))
#+END_SRC

*** Procedures as Returned Values
Procedures can be returned using lambda as follows

#+BEGIN_SRC scheme
(define (get-average-reduction-function-using-function f)
  (lambda (x) 
    (average x (f x))))

((get-average-reduction-function-using-function square) 10)

;;This returns 55, which is the average of 10 and (square 10)
#+END_SRC

*** Abstraction and First Class Procedures
We know that compound procedures are a crucial abstraction mechanism, because they permit us to express general methods
of computing as explicit elements in our programming language e.g. (* x x x) can be represented with a general method
called (cube x). Higher-order procedures permit us to manipulate these general methods to create further abstractions.

As programmers, we should be alert to opportunities to identify the underlying abstractions in our programs and to build
upon them and generalize them to create more powerful abstractions. This is not to say that one should always write
programs in the most abstract way possible; expert programmers know how to choose the level of abstraction appropriate
to their task. But it is important to be able to think in terms of these abstractions, so that we can be ready to apply
them in new contexts. The significance of higher-order procedures is that they enable us to represent these abstractions
explicitly as elements in our programming language, so that they can be handled just like other computational elements.

In general, programming languages impose restrictions on the ways in which computational elements can be
manipulated. Elements with the fewest restrictions are said to have first-class status. Some of the “rights and
privileges” of first-class elements are:

1. They may be named by variables.
2. They may be passed as arguments to procedures.
3. They may be returned as the results of procedures.
4. They may be included in data structures.

*Scheme/Lisp, unlike other common programming languages, awards procedures full first-class status.* This poses
challenges for efficient implementation, but the resulting gain in expressive power is enormous. Chapter 4 highlights
these challenges and provides solutions to deal with them.

*** Notes
- Higher-order procedures don't just allow us to manipulate general methods of computation, they also allow us to reason
  in terms of those general methods thereby enhancing the power of programming langauge to express complex ideas as well
  as the power of programmer to reason about those complex ideas.
- When you create abstractions keep in mind that the deeper they are the more difficult they are for the reader to
  understand. Keep them shallow if possible. I can think of one solution to this problem, let programmers provide as
  deep abstractions as they want while writing, write a tool that unravels the deep abstraction and shows a possible
  expanded view of the same code while reading so that it is easy to understand for a new programmer. Basically the call
  stack has to reside somewhere i.e. either in the brain of programmer or in front of the programmer on the screen, the
  cognitive load on the programmers must be reduced so that they can think in higer levels of abstractions and dive into
  abstractions when necessary. See [[./chapter-1/][chapter-1]], Exercise [[./chapter-2/29-binary-mobile/29-binary-mobile.scm][2.29]].
- If you do have to create deep abstractions ensure that they are named appropriately so that programmer doesnt have to
  unravel them to understand the code, names should be self-explanatory throughout. The cognitive load should be
  reduced. See Exercise [[./chapter-2/29-binary-mobile/29-binary-mobile.scm][2.29]].
- The goal of writing abstractions is to try to arrive at a state where the abstraction can be used as: f(g(h(x),y))
  i.e. a chain of function calls.
- With practice, you should be able to traverse up and down the call tree to manipulate and define the right
  abstraction.
- As the number of higher order functions in the call tree increase, it becomes difficult to keep track of variables so
  ensure that you always use good procedure names and unique variable names so that the code makes sense in
  future. See Exercise [[./chapter-1/42-compose/42-compose.scm][1.46]].

** Miscellaneous
*** Writing Recursive Programs
- Think in terms of smallest/simplest units of computation that is already available or you can achieve, then assemble
  small units to build larger units. Start with the base case and build up.
- Express your computation in as less words as possible, less code leads to less bugs.
- Do not think of simplifying your recursive program to generate an iterative process instead of recursive process
  immediately. First write a program that generates recusive and then improve it so that it generates iterative
  process.
*** Advantages of Recursive Programs
- Compare the amount of code written for recursive/functional programs with equivalent code in procedural programming,
  *When you think differently, you write less code.* 
- You are able to write full fledged programs by only knowing about few core concepts.
- It forces you to write code in small units which ultimately help identify patterns.
- Identified patterns can further be used to define higher-order procedures.


* Chapter 2 : Building Abstractions with Data
Last chapter focused on building abstractions by combining procedures to form compound procedures. This chapter will
focus on another key aspect of building abstractions which involves combining data objects to form compound data.

Why do we want compound data in a programming language?
- For the same reasons that we want compound procedures:
  - to elevate the conceptual level at which we can design our programs,
  - to increase the modularity of our designs,
  - and to enhance the expressive power of our language.

Notes:
- The primary goal of a programming language is to allow the programmer to think better. If the programmer thinks better
  the code will automatically be better. Similary, for software, it shold allow its user to think and therefore work
  better.

** Data Abstraction
The general technique of isolating the parts of a program that deal with how data objects are *represented* from the
parts of a program that deal with how data objects are *used* is called data abstraction.

Programming languages usually provide some primitive data objects and operations that can be used to form compound
data. But *compound data* can also be formed using just procedures which blurs the distinction between data and
procedures. Both should be treated as mechanisms to achieve abstraction.

*** Data Abstraction
- Data abstraction is a methodology that enables us to isolate how a compound data object is used from the details of
  how it is constructed from more primitive data objects.
- Structure the programs that are going to use compound data so that they operate on *abstract data*.
- At the same time, define a *concrete representation* that is independent of its usage.
- The interface between these two parts of our system will be a set of procedures, called *selectors and constructors*,
  that implement the *abstract data* in terms of the *concrete representation*.
- e.g. an implementation of rational numbers.
#+BEGIN_SRC scheme
(define (make-rat n d) (cons n d))
(define (numer x) (car x))
(define (denom x) (cdr x))
#+END_SRC
- An alternative approach that can be implemented later. Data abstraction allows changing it because users are only
  aware of the constructor make-rat, and selectors numer and denom.
#+BEGIN_SRC scheme
(define (make-rat n d)
  (let ((g (gcd n d)))
    (cons (/ n g) 
          (/ d g))))
(define (numer x) (car x))
(define (denom x) (cdr x))
#+END_SRC

*** Abstraction Barriers
- Data abstraction forms levels and procedures at each level are the interfaces that define the *abstraction barriers*
  and connect the different levels.
- The data-abstraction methodology gives us a way to defer some implementation/representation decisions without losing
  the ability to make progress on the rest of the system.
- e.g. we can change above implementation later as follows:
#+BEGIN_SRC scheme
(define (make-rat n d)
  (cons n d))

(define (numer x)
  (let ((g (gcd (car x) (cdr x))))
    (/ (car x) g)))

(define (denom x)
  (let ((g (gcd (car x) (cdr x))))
    (/ (cdr x) g)))
#+END_SRC

*** What Is Meant by Data?
- It is not enough to say “whatever is implemented by the given selectors and constructors.”
- We can think of data as defined by *some collection of selectors and constructors*, *together with specified
  conditions that these procedures must fulfill in order to be a valid representation*.
- For e.g. If we construct a rational number x from a pair of integers n and d, we need to guarantee that extracting the
  numer and the denom of x and dividing them should yield the same result as dividing n by d.
- An alternative implementation of "cons" could be:
#+BEGIN_SRC scheme
(define (cons x y)
  (define (dispatch m)
    (cond ((= m 0) x)
          ((= m 1) y)
          (else 
           (error "Argument not 0 or 1:
                   CONS" m))))
  dispatch)

(define (car z) (z 0))
(define (cdr z) (z 1))
#+END_SRC
- Our implementation of rational numbers is unaffected by this change to "cons".
- Note that this blurs the line between procedures and data. Thus, what matters is whether the constructors and
  selectors satisfy the conditions of the data abstraction not how they are implemented or represented.
- The data representation optimization decisions can be pushed to a later time if required.


** Hierarchical Data and the Closure Property
The ability to create pairs whose elements are pairs is the essence of list structure's importance as a representational
tool.  We refer to this ability as the "closure property" of 'cons'.

Closure is the key to power in any means of combination because it permits us to create "hierarchical"
structures--structures made up of parts, which themselves are made up of parts, and so on.

*** Representing Sequences
Scheme has list operation to represent sequences.

(list <A_1> <A_2> ... <A_N>)

is equivalent to

(cons <A_1> (cons <A_2> (cons ... (cons <A_N> nil) ...)))

**** List Operations
"cdr down and cons up"
#+BEGIN_SRC scheme
(define (length items)
    (if (null? items)
        0
        (+ 1 (length (cdr items)))))

(define (append list1 list2)
    (if (null? list1)
        list2
        (cons (car list1)
              (append (cdr list1) list2))))
#+END_SRC

**** Mapping Over Lists
#+BEGIN_SRC scheme
(define (scale-list items factor)
    (if (null? items)
        nil
        (cons (* (car items) factor)
              (scale-list (cdr items) factor))))
#+END_SRC

We can keep doing "cdr down and cons up" but we can abstract this general idea and capture it as a common pattern
expressed as a higher-order procedure called "map".

#+BEGIN_SRC scheme
(define (map proc items)
    (if (null? items)
        nil
        (cons (proc (car items))
              (map proc (cdr items)))))

(define (scale-list items factor)
    (map (lambda (x) (* x factor))
         items))
#+END_SRC

'Map' is an important construct, not only because it captures a common pattern, but because it establishes a higher
level of  abstraction in dealing with lists.  In the original definition of 'scale-list', the recursive structure of the
program draws attention to the element-by-element processing of the list.  Defining 'scale-list' in terms of 'map'
suppresses that level of detail and emphasizes that scaling transforms a list of elements to a list of results.  The
difference between the two definitions is not that the computer is performing a different process (it isn't) but that we
think about the process differently. In effect, 'map' helps establish an abstraction barrier.

Note: Creating abstraction barriers is important to help programmers think better. The implementation of the abstraction
itself can be optimized later if required. We are writing programs for computers as well as other programmers.

*** Hierarchical Structures
The representation of sequences in terms of lists generalizes naturally to represent sequences whose elements may
themselves be sequences. Another way to think of sequences whose elements are sequences is as "trees".

Recursion is a natural tool for dealing with tree structures, since we can often reduce operations on trees to
operations on their branches, which reduce in turn to operations on the branches of the branches, and so on, until we
reach the leaves of the tree. 

#+BEGIN_SRC scheme
(define (count-leaves x)
    (cond ((null? x) 0)
          ((not (pair? x)) 1)
          (else (+ (count-leaves (car x))
                   (count-leaves (cdr x))))))
#+END_SRC

**** Mapping Over Trees
The recursive plan for 'scale-tree' is similar to the one for 'count-leaves':
#+BEGIN_SRC scheme
(define (scale-tree tree factor)
    ;;(display tree)
    ;;(newline)
    (cond ((null? tree) '())
          ((not (pair? tree)) (* tree factor))
          (else (cons (scale-tree (car tree) factor)
                      (scale-tree (cdr tree) factor)))))
#+END_SRC

Just as 'map' is a powerful abstraction for dealing with sequences, 'map' together with recursion is a powerful
abstraction for dealing with trees. Another way to implement 'scale-tree' is to regard the tree as a sequence of
sub-trees and use 'map'.

#+BEGIN_SRC scheme
;;Adding implementation of map here for reference.
(define (map proc items)
    ;;(display "my-map")
    ;;(display items)
    ;;(newline)
    (if (null? items)
        '()
        (cons (proc (car items))
              (map proc (cdr items)))))

(define (scale-tree tree factor)
   ;;(display "scale-tree-map ")
   ;;(display tree)
   ;;(newline)
   (map (lambda (sub-tree)
            ;;(display "lambda ")
	    ;;(display sub-tree)
	    ;;(newline)
            (if (not (pair? sub-tree))
                (* sub-tree factor)
                (scale-tree sub-tree factor)))
        tree))
#+END_SRC

Note: Both approaches above perform "cdr down cons up" so their performance and tree traversal path are the same;
however, they differ in the way programmer is thinking about the tree traversal problem, in the first approach we are
thinking in terms of pairs and traversing the tree pairwise, in the second we are thinking in terms of subtrees. Note
that in the second approach the lambda passed to map is itself calling "scale-tree" on subtree.

**** Notes
- Recursion is really powerful tool for hierarchical structures. If you name your abstractions properly, all you have to
  do is solve for the base cases and let closure take care of the rest. See Exercise [[./chapter-2/33-accumulate/33-accumulate.scm][2.35]].
*** Sequences as Conventional Interfaces
Use of "conventional interfaces" is a powerful design principle for working with data structures.

In section [[Formulating Abstractions with Higher Order Procedures][1.3]] we saw how program abstractions, implemented as higher-order procedures, can capture *common patterns in
programs* that deal with numerical data. Our ability to formulate analogous operations for working with compound data
depends crucially on the style in which we manipulate our data structures.

#+BEGIN_SRC scheme
(define (sum-odd-squares tree)
    (cond ((null? tree) 0)
          ((not (pair? tree))
          (if (odd? tree) (square tree) 0))
          (else (+ (sum-odd-squares (car tree))
                   (sum-odd-squares (cdr tree))))))

(define (even-fibs n)
    (define (next k)
        (if (> k n)
            nil
            (let ((f (fib k)))
                (if (even? f)
                (cons f (next (+ k 1)))
                (next (+ k 1))))))
    (next 0))
#+END_SRC

Despite the fact that these two procedures are structurally very different, a more abstract description of the two
computations reveals a great deal of similarity.

The first program:
   - enumerates the leaves of a tree;
   - filters them, selecting the odd ones;
   - squares each of the selected ones; and
   - accumulates the results using `+', starting with 0.

The second program:
   - enumerates the integers from 0 to n;
   - computes the Fibonacci number for each integer;
   - filters them, selecting the even ones; and
   - accumulates the results using `cons',  starting with the empty list.

A signal-processing engineer would find it natural to conceptualize these processes in terms of signals flowing through
a cascade of stages. However, our two procedures decompose the computations in a different way, spreading the
enumeration over the program and mingling it with the map, the filter, and the accumulation. If we could organize our
programs to make the signal-flow structure manifest in the procedures we write, this would increase the conceptual
clarity of the resulting code. The key to organizing programs so as to more clearly reflect the signal-flow structure is
to concentrate on the "signals" that flow from one stage in the process to the next. If we represent these signals as
lists, then we can use list operations to implement the processing at each of the stages. Therefore, we can rewrite
sum-odd-squares and even-fibs as:

#+BEGIN_SRC scheme
(define (filter predicate sequence)
  (cond ((null? sequence) nil)
        ((predicate (car sequence))
         (cons (car sequence)
               (filter predicate (cdr sequence))))
        (else (filter predicate (cdr sequence)))))

(define (accumulate op initial sequence)
  (if (null? sequence)
      initial
      (op (car sequence)
          (accumulate op initial (cdr sequence)))))

(define (enumerate-tree tree)
  (cond ((null? tree) nil)
        ((not (pair? tree)) (list tree))
        (else (append (enumerate-tree (car tree))
                      (enumerate-tree (cdr tree))))))

(define (enumerate-interval low high)
  (if (> low high)
      nil
      (cons low (enumerate-interval (+ low 1) high))))

(define (sum-odd-squares tree)
  (accumulate +
              0
              (map square
                   (filter odd?
                           (enumerate-tree tree)))))

(define (even-fibs n)
  (accumulate cons
              nil
              (filter even?
                      (map fib
                           (enumerate-interval 0 n)))))
#+END_SRC

The value of expressing programs as sequence operations is that this helps us make program designs that are modular,
that is, designs that are constructed by combining relatively independent pieces. We can encourage modular design by
providing a library of standard components together with a *conventional interface* for connecting the components in
flexible ways. Modular construction is a powerful strategy for controlling complexity in engineering design.

Sequences, implemented here as lists, serve as a *conventional interface* that permits us to combine processing modules.
Additionally, we can experiment with alternative representations of sequences, while leaving the overall design of our
programs intact.  We will exploit this capability in chapter 3, when we generalize the sequence-processing paradigm to
admit infinite sequences.

Note: Although the time and space complexity may increase from n to 4n or 5n, the resulting conceptual clarity and
modularity helps the programmer understand and write better.

**** Notes
- If you think about it, all operations on a computer are ETL, right from assembly level to the topmost
  abstractions. See Exercise [[./chapter-2/33-accumulate/33-accumulate.scm][2.36]].
- You can obtain efficient solutions to problems with or without abstractions. The difference is that when you solve a
  problem without using any abstractions you start at the most basic level and the solution is as good as your
  logic/imagination/creativity etc. However, when you solve the same problem using available abstractions there are few
  advatages: 1. you think using only the abstractions until the topmost abstraction barrier and might solve the problem
  faster with a easy to understand solution. 2. you benefit from improvements to implementation of abstractions without
  paying in cost. 3. you don't reinvent the wheel, you focus on the problem at hand. 4. you don't solve the problems
  that have already been solved.; and there are some disadvantages: 1. you have to know and learn about the available
  abstractions. See Exercise [[./chapter-2/33-accumulate/33-accumulate.scm][2.36]], [[./chapter-2/37-matrices/37-matrices.scm][2.37]].
- Use post-order/recursion when you want to apply/perform operations on the current item after the result of application
  of operation on the rest of the items/successors is ready. Use pre-order/iteration when you want to apply/perform
  operations on the current item after result from application of operation on previous items/predecessors is
  available. See Exercise [[./chapter-2/38-fold-right-left/38-fold-right-left.scm][2.38]].
- Don't try to remember how a procedure works instead try to remember the properties it provides, the argument it
  accepts and its return type. Thus, clear documentation and naming are critical when defining abstractions.
- Consider the following Spark Scala example. In addition to constructors, selectors and preconditions you should also
  consider providing a procedure that transforms object to itself using the provided function. This helps in chaining
  calls. The ease of readability increases significantly from first to last version.
#+BEGIN_SRC scala
//first attempt
    val rowsWithDummyAudit = addAuditColumnsToRows(rows)
    val unionRows = performUnion(rowsWithDummyAudit, audittedRows)
    val rowsBeforeAsOfDate = removeFutureRows(unionRows, asOfDateTime)
    val rankedRow = rankRows(rowsBeforeAsOfDate)
    val topRankedRows = fetchTopRankedRows(rankedRow)
    val rowsWithoutDeletedRows = removeDeletedRowsBeforeDateTime(topRankedRows, asOfDateTime)
    val finalRows = removeColumns(rowsWithoutDeletedRows, Set(RANK_COL))
    val orderedFinalRows = finalRows.orderBy(orderCriteria map col: _*)

// to chain it.
    val orderedFinalRows2 =
      removeColumns(
        removeDeletedRowsBeforeDateTime(
          fetchTopRankedRows(
            rankRows(
              removeFutureRows(
                performUnion(
                  addAuditColumnsToRows(rows),
                  audittedRows),
                asOfDateTime))),
          asOfDateTime),
        Set(RANK_COL)).orderBy(orderCriteria map col: _*)

// chaining using transform
    val orderedFinalRows3 =
      removeColumns(
        removeDeletedRowsBeforeDateTime(
          removeFutureRows(
            performUnion(rows.transform(addAuditColumnsToRows), audittedRows),
            asOfDateTime)
            .transform(rankRows)
            .transform(fetchTopRankedRows),
          asOfDateTime),
        Set(RANK_COL)).orderBy(orderCriteria map col: _*)

//even better chaining using transform and lambdas
    val orderedFinalRows4 =
      rows.transform(addAuditColumnsToRows)
        .transform(dataFrame => { performUnion(dataFrame, audittedRows) })
        .transform(dataFrame => { removeFutureRows(dataFrame, asOfDateTime) })
        .transform(rankRows)
        .transform(fetchTopRankedRows)
        .transform(dataFrame => { removeDeletedRowsBeforeDateTime(dataFrame, asOfDateTime) })
        .transform(dataFrame => { removeColumns(dataFrame, Set(RANK_COL)) })
        .orderBy(orderCriteria map col: _*)
#+END_SRC

**** Nested Sequences

We can extend the sequence paradigm to include many computations that are commonly expressed using nested
loops. Consider this problem: Given a positive integer n, find all ordered pairs of distinct positive integers i and j,
where 1 <= j< i<= n, such that i + j is prime.

We can generate the sequence of pairs: For each integer i <= n, enumerate the integers j<i, and for each such i and j
generate the pair (i,j).

#+BEGIN_SRC scheme
(accumulate 
    append 
    nil
    (map (lambda (i)
             (map (lambda (j) (list i j))
                  (enumerate-interval 1 (- i 1))))
         (enumerate-interval 1 n)))
#+END_SRC

The combination of mapping and accumulating with 'append' is so common in this sort of program that we will isolate it
as a separate procedure:

#+BEGIN_SRC scheme
     (define (flatmap proc seq)
       (accumulate append nil (map proc seq)))

     ;;Above definition can be used to generate the same sequence of pairs for nested loop as follows:
     (flatmap
         (lambda (i)
             (map (lambda (j) (list i j))
                  (enumerate-interval 1 (- i 1))))
         (enumerate-interval 1 n))
#+END_SRC

All of this can be put together to generate prime-sum-pairs as follows:

#+BEGIN_SRC scheme
(define (prime-sum? pair)
    (prime? (+ (car pair) (cadr pair))))

(define (make-pair-sum pair)
    (list (car pair) (cadr pair) (+ (car pair) (cadr pair))))

(define (prime-sum-pairs n)
  (map make-pair-sum
       (filter prime-sum?
               (flatmap
                   (lambda (i)
                       (map (lambda (j) (list i j))
                            (enumerate-interval 1 (- i 1))))
                   (enumerate-interval 1 n)))))
#+END_SRC

Nested mappings are also useful for sequences other than those that enumerate intervals.  Suppose we wish to generate
all the permutations of a set S {1,2,3}. The idea is to recursively generate the sequence of permutations of S - x, and
adjoin x to the front of each one. This can be written as:

#+BEGIN_SRC scheme
(define (remove item sequence)
  (filter (lambda (x) (not (= x item)))
          sequence))

(define (permutations s)
  (if (null? s)                    ; empty set?
      (list nil)                   ; sequence containing empty set
      (flatmap (lambda (x)
                 (map (lambda (p) (cons x p))
                      (permutations (remove x s))))
               s)))
#+END_SRC

Notice how this strategy reduces the problem of generating permutations of S to the problem of generating the
permutations of sets with fewer elements than S.

*** Example: A Picture Language
- This language is designed to make it easy to experiment with patterns which are composed of repeated elements/images
  that are shifted and scaled.
- The data objects being combined are represented as procedures.
- The operations in this language satisfy the closure property and allow us to easily build arbitrarily complicated
  patterns.
- The elegance of this picture language is that there is only one kind of element, called a painter.
- A painter draws an image that is shifted and scaled to fit within a designated parallelogram-shaped frame. *The painter
  is actually a procedure that returns a procedure which accepts one parameter (the frame). The application of this
  returned procedure draws the painter's image in the provided frame argument with appropriate shifting and scaling to
  fit the frame.*
- To combine images, we use various operations that construct new painters from given painters i.e. the procedures that
  combine painters and produce compound painters accept one or more painters as arguments and return a new painter. We
  are exploiting the fact that painters are closed under the language's means of combination.
- Since the means of combination are ordinary Scheme procedures, we automatically have the capability to do anything
  with painter operations that we can do with procedures i.e. we can create abstractions for a series of operations that
  produce a pattern or we can define recursive operations that produce a pattern.
- Higer-order operations: In addition to abstracting patterns of combining painters, we can work at a higer level,
  abstracting patterns of combining painter operations i.e. procedures that take painter operations as arguments and
  create new painter operations.
- Note: You have to be clearly familiar with the problem domain in order to define the right implementation, abstractions,
  combiners and higher-order procedures. But if you get this right, you can delay the definition of lower levels and
  freely define procedures at higher levels.
- Note: When a procedure returns another procedure it is in effect delaying the execution until the returned procedure
  is applied or until other arguments for returned procedure are available. It is also creating a generic procedure for
  which some arguments are available and other arguments will become available when that procedure is applied.
- Because of the separation of the implementation of the painter from the frame it draws the image in, we can write
  transformers and combiners that need not know how painter works/draws, all they have to know is how to transform or
  combine the frames so that they can pass it to the painer. Similarly, higher-order procedures that operate on
  transformers and combiners need not know details of transformers, combiners, frame or painter, all they need to know
  is how to operate on transformers and combiners to produce the desired patterns.


** Symbolic Data
*** Quotation
- Provides a way to use characters other than numbers. e.g. 'a evaluates to a and 'abc evaluates to abc.
- (list 'a 'b 'c) is equal to '(a b c).
  - And this applies recursively too i.e. (list 'a (list 'b 'c) 'd) is equal to  '(a (b c) d). also equal to (list 'a '(b c) 'd).
  - What is "'(a '(b c) d)"?
  - What is "'abc"?
  - What is "''abc"?
  - What is "'''abc"?
  - What is "'(a (list b c) d)"?
  - What is "'(a (list 1 2) d)"?
  - What is "'wow"?
  - What is "(list 'w 'o 'w)"?
  - What is "'(wow)"?
  - What is "'(w o w)"?
  - What is "(eq? '(wow) (list 'wow))"?
  - What is "(equal? '(wow) (list 'wow))"?
**** Notes
- *Naming* is an important technique to simplify thinking about abstractions. Once you name an abstraction correctly you
  dont have to remember its implementation, you just remember the concept it represents and use it.
- Another important technique is to use *wishful thinking* and write code using abstractions then come back and
  implement the abstractions.


** Multiple Representations for Abstract Data
Data-abstraction barriers are powerful tools for controlling complexity. It allows dividing the task of designing a
large program into smaller tasks that can be performed separately. But this kind of data abstraction is not yet powerful
enough because:
 1. there might be more than one useful representation for a data object, and we might like to design systems that can
   deal with multiple representations.
 2. programming systems are often designed by many people working over extended periods of time, subject to requirements
   that change over time. In such an environment, it is simply not possible for everyone to agree in advance on choices
   of data representation.
 3. large/complex programs are often created by combining pre-existing modules that were designed in isolation.

Therefore, besides data-abstraction barriers that isolate representation from use, we need: 
 1. abstraction barriers that isolate *different design choices* from each other.
 2. permit different choices to *coexist* in a single program.
 3. we need *conventions* that permit programmers to incorporate modules into larger systems *additively*, that is,
    without having to redesign or reimplement these modules.

Following sections describe these mechanisms in detail.

*** Representations for Complex Numbers
We implemented two different representations of complex numbers, namely rectangular and polar. Both have the same
functions i.e. the constructors make-from-real-imag and make-from-mag-ang and the selectors real-part, imag-part,
magnitude, and angle. We can choose to use *one* of these representations to perform operations add-complex,
sub-complex, mul-complex, and div-complex. Thus we have an abstraction barrier that separates representation and usage
of complex numbers. However, we can only chose one representation at a time.

*** Tagged Data
*Principle of least commitment:* The abstraction barrier formed by the selectors and constructors permits us to defer 
to the last possible moment the choice of a concrete representation for our data objects and thus retain maximum
flexibility in our system design.

Tagged data representation lets us take the principle of least commitment even further. Both rectangular and polar
representations get a separate tag during construction allowing abstract selectors real-part, imag-part, magnitude, and
angle pick the right implementation at runtime. Thus we can have both representations at the same time and pick one
based on the tag.

This strategy of checking the type of a datum and calling an appropriate procedure is called *dispatching on type*.

Attaching and detaching tags.

#+BEGIN_SRC scheme
(define (attach-tag type-tag contents)
  (cons type-tag contents))

(define (type-tag datum)
  (if (pair? datum)
      (car datum)
      (error "Bad tagged datum: 
              TYPE-TAG" datum)))

(define (contents datum)
  (if (pair? datum)
      (cdr datum)
      (error "Bad tagged datum: 
              CONTENTS" datum)))
#+END_SRC

Rectangular representation using tagged data.

#+BEGIN_SRC scheme
(define (real-part-rectangular z) (car z))
(define (imag-part-rectangular z) (cdr z))

(define (magnitude-rectangular z)
  (sqrt (+ (square (real-part-rectangular z))
           (square (imag-part-rectangular z)))))

(define (angle-rectangular z)
  (atan (imag-part-rectangular z)
        (real-part-rectangular z)))

(define (make-from-real-imag-rectangular x y)
  (attach-tag 'rectangular (cons x y)))

(define (make-from-mag-ang-rectangular r a)
  (attach-tag 
   'rectangular
   (cons (* r (cos a)) (* r (sin a)))))
#+END_SRC

Generic constructors and selectors.

#+BEGIN_SRC scheme
(define (real-part z)
  (cond ((rectangular? z)
         (real-part-rectangular (contents z)))
        ((polar? z)
         (real-part-polar (contents z)))
        (else (error "Unknown type: 
               REAL-PART" z))))

(define (imag-part z)
  (cond ((rectangular? z)
         (imag-part-rectangular (contents z)))
        ((polar? z)
         (imag-part-polar (contents z)))
        (else (error "Unknown type: 
               IMAG-PART" z))))

(define (magnitude z)
  (cond ((rectangular? z)
         (magnitude-rectangular (contents z)))
        ((polar? z)
         (magnitude-polar (contents z)))
        (else (error "Unknown type: 
               MAGNITUDE" z))))

(define (angle z)
  (cond ((rectangular? z)
         (angle-rectangular (contents z)))
        ((polar? z)
         (angle-polar (contents z)))
        (else (error "Unknown type: 
               ANGLE" z))))

(define (make-from-real-imag x y)
  (make-from-real-imag-rectangular x y))

(define (make-from-mag-ang r a)
  (make-from-mag-ang-polar r a))
#+END_SRC

*** Data-Directed Programming and Additivity
Using tagged data has two significant weaknesses:

1. generic interface procedures (real-part, imag-part, magnitude, and angle) must know about all the different
   representations.
2. even though the individual representations can be designed separately, we must guarantee that no two procedures in
   the entire system have the same name.

The tagged data technique for implementing generic interfaces is not additive. Each addition of a new representation
requires a change in generic interface procedures and programmers have to ensure that their are no name conflicts in
procedures.

Above issues are addressed using data-directed programming. Observe that when dealing with different representations and
their common operations(both constructors and selectors) we are actually dealing with a two dimensional table with
different representations as columns and the operations as rows.

Data-directed programming is the technique of designing programs to work with such a table directly. Individual
representations can be implemented in isolation and then installed into the data-directed system by using put
method. Generic interface procedures use get method to fetch the right concrete procedure for given representation type
and operation name. Thus new representations can be added additively without having to change generic interface
procedure implementation and without worrying about name conflicts. Note that both constructors and selectors can be
added to the table.

(put ⟨op⟩ ⟨type⟩ ⟨item⟩) installs the ⟨item⟩ in the table, indexed by the ⟨op⟩ and the ⟨type⟩.
(get ⟨op⟩ ⟨type⟩) looks up the ⟨op⟩, ⟨type⟩ entry in the table and returns the item found there. If no item is found,
get returns false.

Installing a polar representations package.

#+BEGIN_SRC scheme
(define (install-polar-package)
  ;; internal procedures
  (define (magnitude z) (car z))
  (define (angle z) (cdr z))
  (define (make-from-mag-ang r a) (cons r a))
  (define (real-part z)
    (* (magnitude z) (cos (angle z))))
  (define (imag-part z)
    (* (magnitude z) (sin (angle z))))
  (define (make-from-real-imag x y)
    (cons (sqrt (+ (square x) (square y)))
          (atan y x)))
  ;; interface to the rest of the system
  (define (tag x) (attach-tag 'polar x))
  (put 'real-part '(polar) real-part)
  (put 'imag-part '(polar) imag-part)
  (put 'magnitude '(polar) magnitude)
  (put 'angle '(polar) angle)
  (put 'make-from-real-imag 'polar
       (lambda (x y) 
         (tag (make-from-real-imag x y))))
  (put 'make-from-mag-ang 'polar
       (lambda (r a) 
         (tag (make-from-mag-ang r a))))
  'done)
#+END_SRC

Constructors, selectors and generic procedures.
#+BEGIN_SRC scheme
(define (apply-generic op . args)
  (let ((type-tags (map type-tag args)))
    (let ((proc (get op type-tags)))
      (if proc
          (apply proc (map contents args))
          (error
            "No method for these types: 
             APPLY-GENERIC"
            (list op type-tags))))))

(define (real-part z) 
  (apply-generic 'real-part z))
(define (imag-part z) 
  (apply-generic 'imag-part z))
(define (magnitude z) 
  (apply-generic 'magnitude z))
(define (angle z) 
  (apply-generic 'angle z))

(define (make-from-real-imag x y)
  ((get 'make-from-real-imag 
        'rectangular) 
   x y))

(define (make-from-mag-ang r a)
  ((get 'make-from-mag-ang 
        'polar) 
   r a))
#+END_SRC

*** Message Passing
Systems designed using data-directed programming can be said to have “intelligent operations” that dispatch on data
types. Message passing is a system design technique that creates “intelligent data objects” that dispatch on operation
names. This technique further blurs the distinction between data and code. Notice that We have implemented the notion of
an /object/ with just using procedures.

#+BEGIN_SRC scheme
(define (make-from-real-imag x y)
  (define (dispatch op)
    (cond ((eq? op 'real-part) x)
          ((eq? op 'imag-part) y)
          ((eq? op 'magnitude)
           (sqrt (+ (square x) (square y))))
          ((eq? op 'angle) (atan y x))
          (else
           (error "Unknown op: 
            MAKE-FROM-REAL-IMAG" op))))
  dispatch)

(define (apply-generic op arg) (arg op))
#+END_SRC

*** Notes
- Most of the times we need simple abstraction barriers, the horizontal kind.
- Vertical abstraction barriers are implemented by programmers using one of the above 3 techniques in most programming
  languages. Compare and contrast these techniques e.g. Exercise [[./chapter-2/75-mag-ang/75-mag-ang.scm][2.76]].
- Modern programming languages use all three of the techniques described above and provide flexibility to the
  programmers to define horizontal and vertical barriers.
- Notice that we have not discussed type safety or types of parameters to procedures yet, we have been relying on order
  of parameters and procedure names. But these things will become important as programs grow in complexity.


** Systems with Generic Operations

Last section described how to desgin systems that include multiple representations of data and how to define generic
interface operations that operate any on those representations. This sections describes how to define operations that
are generic over different kinds arguments.

We want to define a uniform interface consisting of operations add, sub, mul, div that can be used by programs operating
on numbers. The numbers however can be of any kind i.e. integer, rational, or complex. We want our system to support
these operations accross or within these kinds of numbers.

Generic interface.
#+BEGIN_SRC scheme
(define (add x y) (apply-generic 'add x y))
(define (sub x y) (apply-generic 'sub x y))
(define (mul x y) (apply-generic 'mul x y))
(define (div x y) (apply-generic 'div x y))

(define (make-scheme-number n)
  ((get 'make 'scheme-number) n))

(define (make-rational n d)
  ((get 'make 'rational) n d))

(define (make-complex-from-real-imag x y)
  ((get 'make-from-real-imag 'complex) x y))
(define (make-complex-from-mag-ang r a)
  ((get 'make-from-mag-ang 'complex) r a))
#+END_SRC

Installation of packages.
#+BEGIN_SRC scheme
(define (install-scheme-number-package)
  (define (tag x)
    (attach-tag 'scheme-number x))
  (put 'add '(scheme-number scheme-number)
       (lambda (x y) (tag (+ x y))))
  (put 'sub '(scheme-number scheme-number)
       (lambda (x y) (tag (- x y))))
  (put 'mul '(scheme-number scheme-number)
       (lambda (x y) (tag (* x y))))
  (put 'div '(scheme-number scheme-number)
       (lambda (x y) (tag (/ x y))))
  (put 'make 'scheme-number
       (lambda (x) (tag x)))
  'done)

(define (install-rational-package)
  ;; internal procedures
  (define (numer x) (car x))
  (define (denom x) (cdr x))
  (define (make-rat n d)
    (let ((g (gcd n d)))
      (cons (/ n g) (/ d g))))
  (define (add-rat x y)
    (make-rat (+ (* (numer x) (denom y))
                 (* (numer y) (denom x)))
              (* (denom x) (denom y))))
  (define (sub-rat x y)
    (make-rat (- (* (numer x) (denom y))
                 (* (numer y) (denom x)))
              (* (denom x) (denom y))))
  (define (mul-rat x y)
    (make-rat (* (numer x) (numer y))
              (* (denom x) (denom y))))
  (define (div-rat x y)
    (make-rat (* (numer x) (denom y))
              (* (denom x) (numer y))))
  ;; interface to rest of the system
  (define (tag x) (attach-tag 'rational x))
  (put 'add '(rational rational)
       (lambda (x y) (tag (add-rat x y))))
  (put 'sub '(rational rational)
       (lambda (x y) (tag (sub-rat x y))))
  (put 'mul '(rational rational)
       (lambda (x y) (tag (mul-rat x y))))
  (put 'div '(rational rational)
       (lambda (x y) (tag (div-rat x y))))
  (put 'make 'rational
       (lambda (n d) (tag (make-rat n d))))
  'done)

(define (install-complex-package)
  ;; imported procedures from rectangular 
  ;; and polar packages
  (define (make-from-real-imag x y)
    ((get 'make-from-real-imag 
          'rectangular) 
     x y))
  (define (make-from-mag-ang r a)
    ((get 'make-from-mag-ang 'polar) 
     r a))
  ;; internal procedures
  (define (add-complex z1 z2)
    (make-from-real-imag 
     (+ (real-part z1) (real-part z2))
     (+ (imag-part z1) (imag-part z2))))
  (define (sub-complex z1 z2)
    (make-from-real-imag 
     (- (real-part z1) (real-part z2))
     (- (imag-part z1) (imag-part z2))))
  (define (mul-complex z1 z2)
    (make-from-mag-ang 
     (* (magnitude z1) (magnitude z2))
     (+ (angle z1) (angle z2))))
  (define (div-complex z1 z2)
    (make-from-mag-ang 
     (/ (magnitude z1) (magnitude z2))
     (- (angle z1) (angle z2))))
  ;; interface to rest of the system
  (define (tag z) (attach-tag 'complex z))
  (put 'add '(complex complex)
       (lambda (z1 z2) 
         (tag (add-complex z1 z2))))
  (put 'sub '(complex complex)
       (lambda (z1 z2) 
         (tag (sub-complex z1 z2))))
  (put 'mul '(complex complex)
       (lambda (z1 z2) 
         (tag (mul-complex z1 z2))))
  (put 'div '(complex complex)
       (lambda (z1 z2) 
         (tag (div-complex z1 z2))))
  (put 'make-from-real-imag 'complex
       (lambda (x y) 
         (tag (make-from-real-imag x y))))
  (put 'make-from-mag-ang 'complex
       (lambda (r a) 
         (tag (make-from-mag-ang r a))))
  'done)
#+END_SRC

Notice that what we have here is a two-level tag system. The outer tag (complex) is used to direct the number to the
complex package. Once within the complex package, the next tag (rectangular or polar) is used to direct the number to the
rectangular or polar package.

The procedures defined/installed so far allow operations only within a type e.g. add interger to an integer.
What we have not yet considered is the fact that it is meaningful to define operations that cross the type boundaries,
such as the addition of a complex number to an ordinary number. We would like to introduce the cross-type operations in
some carefully controlled way, so that we can support them without seriously violating our module boundaries.

One way to handle cross-type operations is to design a different procedure for each possible combination of types for
which the operation is valid. e.g. define add-complex-to-schemenum. There are 4 operations and 3 types. Thus we will
have to define 24 procedures i.e (4 operationts * 6 type permutations (3P2)).

This technique works, but it is cumbersome. With such a system, the cost of introducing a new type is not just the
construction of the package of procedures for that type but also the construction and installation of the procedures
that implement the cross-type operations.The method also undermines our ability to combine separate packages additively.

In the general situation of completely unrelated operations acting on completely unrelated types, implementing explicit
cross-type operations, cumbersome though it may be, is the best that one can hope for. Fortunately, we can usually do
better by taking advantage of additional structure that may be latent in our type system. Often the different data types
are not completely independent, and there may be ways by which objects of one type may be viewed as being of another
type. This process is called *coercion*.

Although we still need to write coercion procedures to relate the types (possibly n^2 procedures for a system with n
types), we need to write only one procedure for each pair of types rather than a different procedure for each collection
of types and each generic operation. i.e. 6(or 9) instead of 24.

#+BEGIN_SRC scheme
(define (apply-generic op . args)
  (let ((type-tags (map type-tag args)))
    (let ((proc (get op type-tags)))
      (if proc
          (apply proc (map contents args))
          (if (= (length args) 2)
              (let ((type1 (car type-tags))
                    (type2 (cadr type-tags))
                    (a1 (car args))
                    (a2 (cadr args)))
                (let ((t1->t2 
                       (get-coercion type1
                                     type2))
                      (t2->t1 
                       (get-coercion type2 
                                     type1)))
                  (cond (t1->t2
                         (apply-generic 
                          op (t1->t2 a1) a2))
                        (t2->t1
                         (apply-generic 
                          op a1 (t2->t1 a2)))
                        (else
                         (error 
                          "No method for 
                           these types"
                          (list 
                           op 
                           type-tags))))))
              (error 
               "No method for these types"
               (list op type-tags)))))))
#+END_SRC

What we actually have is a so-called hierarchy of types (a tower), in which, for example, integers are a subtype of
rational numbers (i.e., any operation that can be applied to a rational number can automatically be applied to an
integer). If we have a tower structure, then we can greatly simplify the problem of adding a new type to the hierarchy,
for we need only specify how the new type is embedded in the next supertype above it and how it is the supertype of the
type below it.

Unfortunately, this is usually not the case. Dealing with large numbers of interrelated types while still preserving
modularity in the design of large systems is very difficult, and is an area of much current research.

*** Notes
- It is important to distinguish between the cases where you need generic operations or generic types while designing a
  system and its behaviour. i.e. horizontal or vertical abstraction.
- At the end of the day, all you need is good abstraction and good implementation/execution of that abstraction. Good
  abstraction makes it easier for the users to understand and use your system. Good implementation/execution provides
  the necessary reliability so that user can have confidence in your system.


* Chapter 3: Modularity, Objects and State

** Assignment and Local State

#+BEGIN_SRC scheme

(define count 0)
(set! count 4)
(define var count)
count ;;4
var ;;4
(set! var 0)
count ;;4
var ;;0 because var and count are not the same.

;;doesnt reset count even if called with (reset-var count), because the parameter count is a name given to the argument count which are two different things in the environment.
(define (reset-var count) (set! count 0))

;;doesnt reset count, because reset-var itself doesnt reset count
(define reset-count (lambda (x) (reset-var count)))

;;resets count to 0 and makes reset-count name map to 4 which is the value returned by set! since it was the old value of count before set to 0. reset-count is a name with value 4 so you cannot apply it. the set! is evaluated when you define reset-count.
(define reset-count (set! count 0))

;;resets count to 0 everytime it is evaluated because it is a name given to a lambda it can be applied/evaluated. it sets the count in the enclosing environment to 0.
(define reset-count (lambda () (set! count 0)))

;;resets count to 0 everytime it is evaluated because it is a procedure with no arguments. it sets the count in the enclosing environment to 0.
(define (reset-count) (set! count 0))

#+END_SRC

*** Notes
- A simple rule you can use in system design is to ask whether you need a *name* or *store* for your value/datym. Using
  *name* implies that it is immutable and we have just assigned a name to a value. *store* on the other hand implies
  that you want to store a value which can change from time to time.
- Mutation itself and the languages allowing that aren't bad, it's a system design option that has its pros and
  cons in terms of time and space complexity, readability, maintainability, abstraction etc. And sometimes the pros of
  system design using mutable data outwiegh the design using immutable data.
- System design with immutable data can also have drawbacks, e.g. information leakage, violation of abstraction
  barriers, additional space or time complexity.


** The Environment Model of Evaluation

*Original Substitution Model of Evaluation*: To apply a compound procedure to arguments, evaluate the body of the
procedure with each formal parameter replaced by the corresponding argument.

However, once we admit assignment into our programming language, such a definition is no longer adequate. In the
presence of assignment, a variable can no longer be considered to be merely a name for a value. Rather, a variable must
somehow designate a “place” in which values can be stored. Therefore, in our new model of evaluation, these places will
be maintained in structures called *environments*.

An environment is a sequence of frames. Each frame is a table (possibly empty) of bindings, which associate variable
names with their corresponding values. (A single frame may contain at most one binding for any variable.) Each frame
also has a pointer to its enclosing environment, unless, for the purposes of discussion, the frame is considered to be
global. The value of a variable with respect to an environment is the value given by the binding of the variable in the
first frame in the environment that contains a binding for that variable. If no frame in the sequence specifies a
binding for the variable, then the variable is said to be unbound in the environment.

The environment determines the context in which an expression should be evaluated. Indeed, one could say that
expressions in a programming language do not, in themselves, have any meaning. Rather, an expression acquires a meaning
only with respect to some environment in which it is evaluated. Even the interpretation of an expression as
straightforward as (+ 1 1) depends on an understanding that one is operating in a context in which + is the symbol for
addition.

*** Rules of Evaluation

The rules of evaluation of a combination remain the same, i.e. to evaluate a combination:
- Evaluate the subexpressions of the combination.
- Apply the value of the operator subexpression to the values of the operand subexpressions.

The environment model of evaluation replaces the substitution model of evaluation in specifying what it means to apply a
compound procedure to arguments. Before we define the environment model of evaluation we have to redefine what is a
procedure. A procedure is always a pair consisting of some code and a pointer to an environment. Procedures are created
in one way only: by evaluating a λ-expression. This produces a procedure whose code is obtained from the text of the
λ-expression and whose environment is the environment in which the λ-expression was evaluated to produce the procedure.

The environment model of evaluation specifies: To apply a procedure to arguments, create a new environment containing a
frame that binds the parameters to the values of the arguments. The enclosing environment of this frame is the
environment specified by the procedure. Now, within this new environment, evaluate the procedure body.

Thus we redefined the procedure and we redefined the context for application of that procedure. The environment model of
procedure application can be summarized by two rules:

1. A procedure object is applied to a set of arguments by constructing a frame, binding the formal parameters of the
   procedure to the arguments of the call, and then evaluating the body of the procedure in the context of the new
   environment constructed. The new frame has as its enclosing environment the environment part of the procedure object
   being applied.
2. A procedure is created by evaluating a λ-expression relative to a given environment. The resulting procedure object
   is a pair consisting of the text of the λ-expression and a pointer to the environment in which the procedure was
   created.

We can now specify the new definitions of *define* and *set!*.
- define: defining a symbol using define creates a binding in the current environment frame and assigns to the symbol
  the indicated value.
- set!: Evaluating the expression (set! ⟨variable⟩ ⟨value⟩) in some environment locates the binding of the variable in
  the environment and changes that binding to indicate the new value. That is, one finds the first frame in the
  environment that contains a binding for the variable and modifies that frame. If the variable is unbound in the
  environment, then set! signals an error.

This affects the operations of simple procedures, ==let==, ==lambda== and internal procedures.


** Modeling with Mutable Data

Compound data is a means for constructing computational objects that have several parts to model real-world objects that
have several aspects. We achieve data abstraction over these compound data objects by using *constructors and
selectors*. Change in state is another aspect of real-world objects which needs to be modeled by computational objects,
this is achieved using *mutators* over compound data. Data objects with mutators are called mutable data objects.

*** Mutable List Structure
Pairs are a general-purpose “glue” for synthesizing compound data as seen in previous chapters. This section describes
basic mutators for pairs, so that pairs can serve as building blocks for constructing mutable data objects.

The primitive mutators for pairs are set-car! and set-cdr!.

One way to detect sharing in list structures is to use the predicate eq?. ==(eq? x y)== tests whether x and y are the
same object (that is, whether x and y are equal as pointers).

We can exploit sharing to *greatly extend the repertoire of data structures* that can be represented by pairs. On the
other hand, sharing can also be dangerous, since modifications made to structures will also affect other structures that
happen to share the modified parts.

Mutation is just assignment. We can implement mutable data objects as procedures using assignment and local state as
follows:

#+BEGIN_SRC scheme
(define (cons x y)
  (define (set-x! v) (set! x v))
  (define (set-y! v) (set! y v))
  (define (dispatch m)
    (cond ((eq? m 'car) x)
          ((eq? m 'cdr) y)
          ((eq? m 'set-car!) set-x!)
          ((eq? m 'set-cdr!) set-y!)
          (else (error "Undefined 
                 operation: CONS" m))))
  dispatch)

(define (car z) (z 'car))
(define (cdr z) (z 'cdr))

(define (set-car! z new-value)
  ((z 'set-car!) new-value)
  z)

(define (set-cdr! z new-value)
  ((z 'set-cdr!) new-value)
  z)
#+END_SRC

Assignment is all that is needed, theoretically, to account for the behavior of mutable data.

**** Notes
- Mutators along with constructors and selectors now define the operations on abstract data. Mutators along with
  constructors now allow the ability to change data.
- By introducing just one extra operation of set! we raise all the issues, not only of assignment, but of mutable data
  in general. Therefore, choose between a immutable or mutable design wisely. Both have pros and cons.
- When you use message passing i.e. dispatch to define an abstract data type, you have to define the procedures twice;
  once within the abstract data type definition and once outside. The outside procedures always accepts the data object
  of that type itself as the first argument and then reroutes the request with appropriate message. e.g. check how car,
  cdr, set-car! and set-cdr! are defined.


*** Representing Queues and Tables
The mutators set-car! and set-cdr! enable us to use pairs to construct data structures that cannot be built with cons,
car, and cdr alone. e.g. Queue, Table etc. 

For example, in terms of data abstraction, we can regard a queue as defined by the following set of operations:
1. a constructor: (make-queue) returns an empty queue (a queue containing no items).
2. two selectors:
   1. (empty-queue? ⟨queue⟩): tests if the queue is empty.
   2. (front-queue ⟨queue⟩): returns the object at the front of the queue, signaling an error if the queue is empty; it
      does not modify the queue.
3. two mutators:
   1. (insert-queue! ⟨queue⟩ ⟨item⟩): inserts the item at the rear of the queue and returns the modified queue as its
      value.
   2. (delete-queue! ⟨queue⟩): removes the item at the front of the queue and returns the modified queue as its value,
      signaling an error if the queue is empty before the deletion.

Because a queue is a sequence of items, we could certainly represent it as an ordinary list; however, this
representation is inefficient, because the insert-queue operation takes Θ(n) steps. If we allow mutation, we can
represent the queue as a list, together with an additional pointer that indicates the final pair in the list, the
insert-queue operation now takes Θ(1) steps. A queue is represented, then, as a pair of pointers, front-ptr and
rear-ptr. Table can be implemented similarly.

**** Notes
- Higher order procedures are sometimes difficult to understand. e.g. Exercise 3.27
#+BEGIN_SRC scheme
(define memo-fib
  (memoize 
   (lambda (n)
     (cond ((= n 0) 0)
           ((= n 1) 1)
           (else 
            (+ (memo-fib (- n 1))
               (memo-fib (- n 2))))))))

(define (memoize f)
  (let ((table (make-table)))
    (lambda (x)
      (let ((previously-computed-result 
             (lookup x table)))
        (or previously-computed-result
            (let ((result (f x)))
              (insert! x result table)
              result))))))
#+END_SRC
This involves several things. 1. It is a two level recursion. 2. memo-fib is a name given to a procedure returned by
memoize, Scheme should really have a way to distinguish between names given to data(e.g. define-data), names given to
directly defined procedures(e.g. define-proc), and names given to lambdas(e.g. define-lambda).
#+BEGIN_SRC scheme
(define-lambda memo-fib
  (memoize 
   (lambda (n)
     (cond ((= n 0) 0)
           ((= n 1) 1)
           (else 
            (+ (memo-fib (- n 1))
               (memo-fib (- n 2))))))))

(define-proc (memoize f)
  (let ((table (make-table)))
    (lambda (x)
      (let ((previously-computed-result 
             (lookup x table)))
        (or previously-computed-result
            (let ((result (f x)))
              (insert! x result table)
              result))))))
#+END_SRC
This strategy however, raises an issue, the way parameters are attached to arguments of different types (i.e. data,
procedures, lambdas) is the same, here you can't distinguish between them like we did with define-data, define-proc,
define-lambda, this will make the language inconsistent. 
Another way to get over this difficulty is using types. e.g. (define type-data abc 1) and (define type-lambda f (lamdba
(x) (<body>))). We want something that pops out immediately while the programmer is reading the code/definition.
With restrictions in Scheme, we can get over this difficulty by following naming conventions for names given to lambdas
(prefix with lambda-) and procedures that return lambda (prefix with make-lambda-).
#+BEGIN_SRC scheme
(define lambda-memo-fib
  (make-lambda-memoize
   (lambda (n)
     (cond ((= n 0) 0)
           ((= n 1) 1)
           (else 
            (+ (lambda-memo-fib (- n 1))
               (lambda-memo-fib (- n 2))))))))

(define (make-lambda-memoize lambda-to-memoize)
  (let ((table (make-table)))
    (lambda (x)
      (let ((previously-computed-result 
             (lookup x table)))
        (or previously-computed-result
            (let ((result (lambda-to-memoize x)))
              (insert! x result table)
              result))))))
#+END_SRC


*** Simulator for Digital Circuits
- Exercise 3.28 to 3.30. Some objects in real world need to be modeled with mutable data, note how the inverter is
  defined, it is an operation that doesn't construct or return a wire, it accepts output wire as argument, this wire is
  modeled with mutable data which changes state over time. So while designing systems, along with asking whether you
  need a name or a store for value, you can also ask does the value change over time and if so, how.
- Implementing "agenda": don't bring the notion of actual time in your system design if it is not required. Design at an
  abstract level without notion of actual time as much as you can, introduce actual time when absolutely necessary.


*** Propagation of Constraints
- Although the primitives, means of combination, and abstraction are beautifully defined in the constraint system, as
  shown by Exercise [[./chapter-3/33-constraint-system/33-constraint-system.scm][3.34]], the programmer cannot simply use the primitives or abstractions, they do have to be aware of
  the implementation or they will make mistakes.
- It is difficult to keep track of stacks of calls and branches in the mind. It is easy to look at a sequence of
  instructions and determine whats going on in code.
- Check Exercise [[./chapter-3/33-constraint-system/33-constraint-system.scm][3.37]]. Implementation of c- and c/ is ingenious.
- How would you achieve propagation of constraints in object-oriented programming?


** Concurrency: Time Is of the Essence

Introducing set! in our computational model raised the questions about state(local or global), sameness and change. But
the central issue behind this is that by introducing assignment we also introduced *time* in our computational
model. Thus we are no longer dealing with value of x but we are dealing with value of x(t) which then forces us to
consider *order* in which operations are performed which wasn't a concern before.

However problematic, we cannot get around this problem, we have to manage and address it because that is how objects in
the real world behave and interact with each other. If we are to model objects in physical world into computational
models we have to admit time into our models. Objects in the world do not change one at a time in sequence. Rather we
*perceive* them as acting concurrently—all at once. So it is often natural and sometimes essential to model systems as
collections of computational processes that execute concurrently.

Just as we can make our programs modular by organizing models in terms of objects with separate local state, it is often
appropriate to divide computational models into parts that evolve separately and concurrently. Even if the programs are
to be executed on a sequential computer, the practice of writing programs as if they were to be executed concurrently
forces the programmer to avoid inessential timing constraints and thus makes programs more modular. Thus modeling for
concurrency is not just a system requirement it is also essential to make our programs modular.

Broadly there are three ways of modeling systems: immutable, mutable, concurrent. You should determine what kind of
system you are modeling and design code accordingly. Immutable and immutable-concurrent modeling is relatively easy to
code and think. Just mutable modeling is also easy if it doesn't involve concurrency considerations. But even a single
processing computer involves pipelining, caching etc at lower level therefore mutable-concurrent modeling is essential,
so design mutable code from the get go such that it is ready to handle concurrency. Concurrent processes are not a
problem themselves, issues arise when concurrent processes either share some state or they have to communicate or
interact.

In addition to making programs more modular, concurrent computation can provide a speed advantage over sequential
computation.

*** The Nature of Time in Concurrent Systems

To make concurrent programs behave correctly, we may have to place some restrictions on concurrent execution.

One possible restriction on concurrency would stipulate that no two operations that change any shared state variables
can occur at the same time. This is an extremely stringent requirement. A less stringent restriction on concurrency
would ensure that a concurrent system produces the same result as if the processes had run sequentially in some
order. There are two important aspects to this requirement. First, it does not require the processes to actually run
sequentially, but only to produce results that are the same as if they had run sequentially. Second, there may be more
than one possible “correct” result produced by a concurrent program, because we require only that the result be the same
as for some sequential order. A more formal way to express this idea is to say that concurrent programs are inherently
nondeterministic. That is, they are described not by single-valued functions, but by functions whose results are sets of
possible values.


*** Mechanisms for Controlling Concurrency

The difficulty in dealing with concurrent processes is rooted in the need to consider the interleaving of the order of
events in the different processes. Suppose we have two processes, one with three ordered events (a,b,c) and one with
three ordered events (x,y,z). If the two processes run concurrently, with no constraints on how their execution is
interleaved, then there are 20 different possible orderings for the events that are consistent with the individual
orderings for the two processes.

As programmers designing this system, we would have to consider the effects of each of these 20 orderings and check that
each behavior is acceptable. Such an approach rapidly becomes unwieldy as the numbers of processes and events increase.

A more practical approach to the design of concurrent systems is to devise general mechanisms that allow us to constrain
the interleaving of concurrent processes so that we can be sure that the program behavior is correct. Many mechanisms
have been developed for this purpose. In this section, we describe one of them, the serializer.

Serialization implements the following idea: Processes will execute concurrently, but there will be certain collections
of procedures that cannot be executed concurrently. More precisely, serialization creates distinguished sets of
procedures such that only one execution of a procedure in each *serialized set* is permitted to happen at a time.

We can use serialization to control access to shared variables. For example, if we want to update a shared variable
based on the previous value of that variable, we put the access to the previous value of the variable and the assignment
of the new value to the variable in the same procedure. We then ensure that no other procedure that assigns to the
variable can run concurrently with this procedure by serializing all of these procedures with the same serializer. This
guarantees that the value of the variable cannot be changed between an access and the corresponding assignment.

However, while using serializers is relatively straightforward when there is only a single shared resource, concurrent
programming can be treacherously difficult when there are *multiple shared resources*.

We implement serializers in terms of a more primitive synchronization mechanism called a mutex. A mutex is an object
that supports two operations—the mutex can be acquired, and the mutex can be released. One such mechanism is
*test-and-set!*. The actual implementation of test-and-set! depends on the details of how our system runs concurrent 
processes. For example, we might be executing concurrent processes on a sequential processor using a time-slicing
mechanism that cycles through the processes, permitting each process to run for a short time before interrupting it and
moving on to the next process. In that case, test-and-set! can work by disabling time slicing during the testing and
setting. Alternatively, multiprocessing computers provide instructions that support atomic operations directly in
hardware.

There are many variants of such instructions including *test-and-set, test-and-clear, swap, compare-and-exchange,
load-reserve, and store-conditional* whose design must be carefully matched to the machine’s processor-memory
interface. One issue that arises here is to determine what happens if two processes attempt to acquire the same resource
at exactly the same time by using such an instruction. This requires some mechanism for making a decision about which
process gets control. Such a mechanism is called an *arbiter*. Arbiters usually boil down to some sort of hardware
device. Unfortunately, it is possible to prove that one cannot physically construct a fair arbiter that works 100% of
the time unless one allows the arbiter an arbitrarily long time to make its decision.

Mechanisms such as test-and-set! require processes to examine a global shared flag at arbitrary times. This is
problematic and inefficient to implement in modern high-speed processors, where due to optimization techniques such as
pipelining and cached memory, the contents of memory may not be in a consistent state at every instant. In contemporary
multiprocessing systems, therefore, the serializer paradigm is being supplanted by new approaches to concurrency
control.

One such alternative to serialization is called *barrier synchronization*. The programmer permits concurrent processes
to execute as they please, but establishes certain synchronization points (“barriers”) through which no process can
proceed until all the processes have reached the barrier. Modern processors provide machine instructions that permit
programmers to establish synchronization points at places where consistency is required. The PowerPC, for example,
includes for this purpose two instructions called SYNC and EIEIO (Enforced In-order Execution of Input/Output).

The problematic aspects of shared state also arise in large, distributed systems. The basic phenomenon here is that
synchronizing different processes, establishing shared state, or imposing an order on events requires communication
among the processes. In essence, any notion of time in concurrency control must be intimately tied to communication. For
distributed systems, this perspective was pursued by Lamport (1978), who showed how to use communication to establish
“global clocks” that can be used to establish orderings on events in distributed systems.

It is intriguing that a similar connection between time and communication also arises in the Theory of Relativity, where
the speed of light (the fastest signal that can be used to synchronize events) is a fundamental constant relating time
and space. The complexities we encounter in dealing with time and state in our computational models may in fact mirror a
fundamental complexity of the physical universe.


*** Notes
- e.g. [[./utilities/counting.scm][permutations]], while doing functional programming where you build up on top of lower layers, you have to remember
  a lot of idioms, their definitions, and how they are implemented. How many of these can an average programmer remember?
- check [[./chapter-3/section-3-4-2/section-3-4-2.scm][Section 3.4.2 formula.]]


* Miscellaneous


* References
- https://mitpress.mit.edu/sites/default/files/sicp/index.html
- https://sarabander.github.io/sicp/html/index.xhtml
- http://zv.github.io/sicp-in-texinfo
- https://xuanji.appspot.com/isicp/index.html
- https://www.neilvandyke.org/sicp-texi/
- https://github.com/webframp/sicp-info


* SICP Info
** Install
#+BEGIN_SRC bash
mv ~/Downloads/sicp.info /usr/local/share/info/
install-info /usr/local/share/info/sicp.info --dir-file="/usr/local/share/info/dir"
#+END_SRC
** Usage
*** Option 1
- Open file "/usr/local/share/info/sicp.info" in emacs.
- emacs will open it in info mode.
*** Option 2
- "C-h i"
- This will open info InfoMode, search for SICP.
